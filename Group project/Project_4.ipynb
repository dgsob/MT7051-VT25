{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "3tRjgyPxl53U",
        "aju8u04MB-Bv"
      ],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPlo5FuqxQuG5/k/rymNyFe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dgsob/MT7051-VT25/blob/main/Group%20project/Project_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Environment"
      ],
      "metadata": {
        "id": "biPskFDRGv_5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MuJoCo engine set up"
      ],
      "metadata": {
        "id": "3tRjgyPxl53U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install mujoco\n",
        "!pip install mujoco\n",
        "\n",
        "# Set up GPU rendering.\n",
        "from google.colab import files\n",
        "import distutils.util\n",
        "import os\n",
        "import subprocess\n",
        "\n",
        "# Check if GPU is available\n",
        "if subprocess.run('nvidia-smi').returncode:\n",
        "  raise RuntimeError(\n",
        "      'Cannot communicate with GPU. '\n",
        "      'Make sure you are using a GPU Colab runtime. '\n",
        "      'Go to the Runtime menu and select Choose runtime type.')\n",
        "\n",
        "# Add an ICD config for Nvidia EGL driver\n",
        "NVIDIA_ICD_CONFIG_PATH = '/usr/share/glvnd/egl_vendor.d/10_nvidia.json'\n",
        "if not os.path.exists(NVIDIA_ICD_CONFIG_PATH):\n",
        "  with open(NVIDIA_ICD_CONFIG_PATH, 'w') as f:\n",
        "    f.write(\"\"\"{\n",
        "    \"file_format_version\" : \"1.0.0\",\n",
        "    \"ICD\" : {\n",
        "        \"library_path\" : \"libEGL_nvidia.so.0\"\n",
        "    }\n",
        "}\n",
        "\"\"\")\n",
        "\n",
        "# Configure MuJoCo to use the EGL rendering backend (requires GPU)\n",
        "print('Setting environment variable to use GPU rendering:')\n",
        "%env MUJOCO_GL=egl\n",
        "\n",
        "# Check if installation was successful\n",
        "try:\n",
        "  print('Checking that the installation succeeded:')\n",
        "  import mujoco\n",
        "  mujoco.MjModel.from_xml_string('<mujoco/>')\n",
        "except Exception as e:\n",
        "  raise e from RuntimeError(\n",
        "      'Something went wrong during installation. Check the shell output above '\n",
        "      'for more information.\\n'\n",
        "      'If using a hosted Colab runtime, make sure you enable GPU acceleration '\n",
        "      'by going to the Runtime menu and selecting \"Choose runtime type\".')\n",
        "\n",
        "print('Installation successful.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Wfi_xhngw0_",
        "outputId": "2428a09c-ffe6-479e-dd90-18c886a8be44"
      },
      "execution_count": 188,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: mujoco in /usr/local/lib/python3.11/dist-packages (3.1.6)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from mujoco) (1.4.0)\n",
            "Requirement already satisfied: etils[epath] in /usr/local/lib/python3.11/dist-packages (from mujoco) (1.12.2)\n",
            "Requirement already satisfied: glfw in /usr/local/lib/python3.11/dist-packages (from mujoco) (2.8.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from mujoco) (2.0.2)\n",
            "Requirement already satisfied: pyopengl in /usr/local/lib/python3.11/dist-packages (from mujoco) (3.1.9)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from etils[epath]->mujoco) (2024.10.0)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.11/dist-packages (from etils[epath]->mujoco) (6.5.2)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.11/dist-packages (from etils[epath]->mujoco) (4.12.2)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.11/dist-packages (from etils[epath]->mujoco) (3.21.0)\n",
            "Setting environment variable to use GPU rendering:\n",
            "env: MUJOCO_GL=egl\n",
            "Checking that the installation succeeded:\n",
            "Installation successful.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Imports"
      ],
      "metadata": {
        "id": "aju8u04MB-Bv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Gym\n",
        "!pip install gymnasium\n",
        "!pip install gymnasium-robotics\n",
        "import gymnasium as gym\n",
        "import gymnasium_robotics as robotics\n",
        "\n",
        "# Rendering\n",
        "!apt-get install -y xvfb\n",
        "!pip install imageio ffmpeg pyvirtualdisplay\n",
        "import imageio\n",
        "from IPython.display import display\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "\n",
        "# Train\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import random\n",
        "import copy\n",
        "from collections import namedtuple\n",
        "\n",
        "# Save the agent\n",
        "import os\n",
        "import pickle"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "34yIvql3mAWF",
        "outputId": "5f7aa129-2a6e-49f2-85f9-496c1839d794"
      },
      "execution_count": 189,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gymnasium in /usr/local/lib/python3.11/dist-packages (1.1.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (2.0.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (0.0.4)\n",
            "Requirement already satisfied: gymnasium-robotics in /usr/local/lib/python3.11/dist-packages (1.3.1)\n",
            "Requirement already satisfied: mujoco<3.2.0,>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium-robotics) (3.1.6)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium-robotics) (2.0.2)\n",
            "Requirement already satisfied: gymnasium>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium-robotics) (1.1.1)\n",
            "Requirement already satisfied: PettingZoo>=1.23.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium-robotics) (1.24.3)\n",
            "Requirement already satisfied: Jinja2>=3.0.3 in /usr/local/lib/python3.11/dist-packages (from gymnasium-robotics) (3.1.6)\n",
            "Requirement already satisfied: imageio in /usr/local/lib/python3.11/dist-packages (from gymnasium-robotics) (2.37.0)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium>=1.0.0->gymnasium-robotics) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium>=1.0.0->gymnasium-robotics) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium>=1.0.0->gymnasium-robotics) (0.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from Jinja2>=3.0.3->gymnasium-robotics) (3.0.2)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from mujoco<3.2.0,>=2.2.0->gymnasium-robotics) (1.4.0)\n",
            "Requirement already satisfied: etils[epath] in /usr/local/lib/python3.11/dist-packages (from mujoco<3.2.0,>=2.2.0->gymnasium-robotics) (1.12.2)\n",
            "Requirement already satisfied: glfw in /usr/local/lib/python3.11/dist-packages (from mujoco<3.2.0,>=2.2.0->gymnasium-robotics) (2.8.0)\n",
            "Requirement already satisfied: pyopengl in /usr/local/lib/python3.11/dist-packages (from mujoco<3.2.0,>=2.2.0->gymnasium-robotics) (3.1.9)\n",
            "Requirement already satisfied: pillow>=8.3.2 in /usr/local/lib/python3.11/dist-packages (from imageio->gymnasium-robotics) (11.1.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from etils[epath]->mujoco<3.2.0,>=2.2.0->gymnasium-robotics) (2024.10.0)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.11/dist-packages (from etils[epath]->mujoco<3.2.0,>=2.2.0->gymnasium-robotics) (6.5.2)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.11/dist-packages (from etils[epath]->mujoco<3.2.0,>=2.2.0->gymnasium-robotics) (3.21.0)\n",
            "E: dpkg was interrupted, you must manually run 'dpkg --configure -a' to correct the problem. \n",
            "Requirement already satisfied: imageio in /usr/local/lib/python3.11/dist-packages (2.37.0)\n",
            "Requirement already satisfied: ffmpeg in /usr/local/lib/python3.11/dist-packages (1.4)\n",
            "Requirement already satisfied: pyvirtualdisplay in /usr/local/lib/python3.11/dist-packages (3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from imageio) (2.0.2)\n",
            "Requirement already satisfied: pillow>=8.3.2 in /usr/local/lib/python3.11/dist-packages (from imageio) (11.1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define the environment"
      ],
      "metadata": {
        "id": "PPHvddEGRv84"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "REACH = \"FetchReach-v3\" # easiest one, arm reaches to a point\n",
        "PICK_AND_PLACE = \"FetchPickAndPlace-v3\" # hardest one, arm reaches for the block, grabs it, reaches to a point"
      ],
      "metadata": {
        "id": "DtS59dk_pq3r"
      },
      "execution_count": 190,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_env(env_name=\"FetchReach-v3\"):\n",
        "    \"\"\"Creates and returns a Gymnasium environment.\n",
        "\n",
        "    Args:\n",
        "        env_name (string, optional): The name of the environment to create.\n",
        "            Defaults to: \"FetchReach-v3\"\n",
        "            Valid options are:\n",
        "                - \"FetchReach-v3\"\n",
        "                - \"FetchPickAndPlace-v3\"\n",
        "\n",
        "    Returns:\n",
        "        gym.Env: The created Gymnasium environment.\n",
        "\n",
        "    Prints:\n",
        "        Observation space: The observation space of the environment.\n",
        "        Action space: The action space of the environment.\n",
        "    \"\"\"\n",
        "    env = gym.make(env_name, render_mode=\"rgb_array\")\n",
        "    print(\"Observation space:\", env.observation_space)\n",
        "    print(\"Action space:\", env.action_space)\n",
        "    return env"
      ],
      "metadata": {
        "id": "HBTa20V7Cnrw"
      },
      "execution_count": 191,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run a random agent to test the env (commented out)"
      ],
      "metadata": {
        "id": "BG1Qi55FTUXW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# def run_random_actions(env, steps):\n",
        "#     \"\"\"Run a random agent in the environment for a given number of steps.\"\"\"\n",
        "#     frames = []\n",
        "#     observation, info = env.reset()\n",
        "\n",
        "#     for _ in range(steps):\n",
        "#         action = env.action_space.sample()  # Sample a random action\n",
        "#         obs, reward, terminated, truncated, info = env.step(action)\n",
        "\n",
        "#         # Capture frame\n",
        "#         frames.append(env.render())\n",
        "\n",
        "#         # Stop if episode ends\n",
        "#         if terminated or truncated:\n",
        "#             break\n",
        "\n",
        "#     env.close()\n",
        "#     return frames"
      ],
      "metadata": {
        "id": "7ZD6OCBgcaBt"
      },
      "execution_count": 192,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Run everything\n",
        "# env = create_env()\n",
        "# frames = run_random_actions(env, steps=1000)\n",
        "# imageio.mimsave(\"random_actions_fetch.gif\", frames, fps=30)"
      ],
      "metadata": {
        "id": "lRg-aK2ZjnF-"
      },
      "execution_count": 193,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "q23EscCUG5-f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Device configuration (cuda)"
      ],
      "metadata": {
        "id": "MwrIA88ULCV1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "id": "PxCveEDglZzN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a91ea359-8fb5-46c9-d839-187393a26fd7"
      },
      "execution_count": 194,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Neural Networks"
      ],
      "metadata": {
        "id": "Ca8qiz87J9sv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuration from the fetch paper\n",
        "# Define Actor network\n",
        "class Actor(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim, max_action):\n",
        "        super(Actor, self).__init__()\n",
        "        self.max_action = max_action\n",
        "\n",
        "        self.fc1 = nn.Linear(state_dim, 256)\n",
        "        self.fc2 = nn.Linear(256, 256)\n",
        "        self.fc3 = nn.Linear(256, 256)\n",
        "        self.out = nn.Linear(256, action_dim)\n",
        "\n",
        "        self.out.weight.data.uniform_(-3e-3, 3e-3)\n",
        "        self.out.bias.data.uniform_(-3e-3, 3e-3)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = F.relu(self.fc3(x))\n",
        "        x = torch.tanh(self.out(x)) * self.max_action\n",
        "        return x\n",
        "\n",
        "# Define Critic network\n",
        "class Critic(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim):\n",
        "        super(Critic, self).__init__()\n",
        "\n",
        "        self.fc1 = nn.Linear(state_dim, 256)\n",
        "        self.fc2 = nn.Linear(256 + action_dim, 256)\n",
        "        self.fc3 = nn.Linear(256, 256)\n",
        "        self.out = nn.Linear(256, 1)\n",
        "\n",
        "        self.out.weight.data.uniform_(-3e-4, 3e-4)\n",
        "        self.out.bias.data.uniform_(-3e-4, 3e-4)\n",
        "\n",
        "    def forward(self, state, action):\n",
        "        xs = F.relu(self.fc1(state))\n",
        "        x = torch.cat([xs, action], dim=1)\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = F.relu(self.fc3(x))\n",
        "        x = self.out(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "M-Z45KTIERWa"
      },
      "execution_count": 195,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ornstein-Uhlenbeck noise process for temporally correlated exploration"
      ],
      "metadata": {
        "id": "Q-lsdpmMn3nI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class OrnsteinUhlenbeckNoise:\n",
        "    \"\"\"\n",
        "    Ornstein-Uhlenbeck process for temporally correlated exploration.\n",
        "\n",
        "    Parameters:\n",
        "    - action_dimension: Dimension of the action space\n",
        "    - mu: Mean of the noise\n",
        "    - theta: How quickly the process reverts to the mean (larger -> faster)\n",
        "    - sigma: Scale of the noise\n",
        "    \"\"\"\n",
        "    def __init__(self, action_dimension, mu=0, theta=0.15, sigma=0.2):\n",
        "        self.action_dimension = action_dimension\n",
        "        self.mu = mu\n",
        "        self.theta = theta\n",
        "        self.sigma = sigma\n",
        "        self.state = np.ones(self.action_dimension) * self.mu\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"Reset the internal state to the mean.\"\"\"\n",
        "        self.state = np.ones(self.action_dimension) * self.mu\n",
        "\n",
        "    def sample(self):\n",
        "        \"\"\"Generate a sample of noise.\"\"\"\n",
        "        x = self.state\n",
        "        dx = self.theta * (self.mu - x) + self.sigma * np.random.randn(self.action_dimension)\n",
        "        self.state = x + dx\n",
        "        return self.state"
      ],
      "metadata": {
        "id": "2gqK3pHwn2fI"
      },
      "execution_count": 196,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Implementation: *HER*"
      ],
      "metadata": {
        "id": "puhYIp30KFXN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Replay buffer with HER\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, max_size=1000000):\n",
        "        self.buffer = []\n",
        "        self.max_size = max_size # from fetch paper\n",
        "        self.ptr = 0\n",
        "\n",
        "        # For storing entire episodes for HER\n",
        "        self.episode_buffer = []\n",
        "\n",
        "    def add(self, state, action, next_state, reward, done, goal):\n",
        "        if len(self.buffer) < self.max_size:\n",
        "            self.buffer.append(None)\n",
        "\n",
        "        self.buffer[self.ptr] = (state, action, next_state, reward, done, goal)\n",
        "        self.ptr = (self.ptr + 1) % self.max_size\n",
        "\n",
        "    def add_episode_step(self, observation, action, next_observation, reward, done):\n",
        "        \"\"\"Store a step in the episode buffer with full observation dictionary\"\"\"\n",
        "        self.episode_buffer.append((observation, action, next_observation, reward, done))\n",
        "\n",
        "    def process_episode_with_her(self, env, k=4):\n",
        "        \"\"\"Process episode with HER, using the Gym-robotics style API\"\"\"\n",
        "        episode_len = len(self.episode_buffer)\n",
        "\n",
        "        # First, add the original experience to the replay buffer\n",
        "        for i in range(episode_len):\n",
        "            obs_dict, action, next_obs_dict, reward, done = self.episode_buffer[i]\n",
        "\n",
        "            # Original experience with intended goal\n",
        "            self.store_transition(obs_dict, action, next_obs_dict, reward, done)\n",
        "\n",
        "            # HER: For k random future states - this is a future strategy implementation from the HER paper\n",
        "            if k > 0:\n",
        "                future_indices = np.random.randint(i, episode_len, size=min(k, episode_len-i))\n",
        "\n",
        "                for future_idx in future_indices:\n",
        "                    future_obs_dict = self.episode_buffer[future_idx][2]  # Get the next observation at future step\n",
        "                    achieved_goal = future_obs_dict[\"achieved_goal\"]\n",
        "\n",
        "                    # Use the future achieved goal as the desired goal\n",
        "                    her_reward = env.unwrapped.compute_reward(\n",
        "                        next_obs_dict[\"achieved_goal\"],\n",
        "                        achieved_goal,\n",
        "                        info={}\n",
        "                    )\n",
        "\n",
        "                    # Store the transition with modified goal\n",
        "                    self.store_transition(\n",
        "                        obs_dict,\n",
        "                        action,\n",
        "                        next_obs_dict,\n",
        "                        her_reward,\n",
        "                        done,\n",
        "                        new_goal=achieved_goal\n",
        "                    )\n",
        "\n",
        "        # Clear episode buffer for next episode\n",
        "        self.episode_buffer = []\n",
        "\n",
        "    def store_transition(self, obs_dict, action, next_obs_dict, reward, done, new_goal=None):\n",
        "        \"\"\"Store a processed transition in the replay buffer\"\"\"\n",
        "        # If new_goal is provided, use it instead of the original desired_goal\n",
        "        goal = new_goal if new_goal is not None else obs_dict[\"desired_goal\"]\n",
        "\n",
        "        # Create combined state vector\n",
        "        state = np.concatenate([obs_dict[\"observation\"], goal])\n",
        "        next_state = np.concatenate([next_obs_dict[\"observation\"], goal])\n",
        "\n",
        "        # Add to buffer\n",
        "        self.add(state, action, next_state, reward, done, goal)\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        ind = np.random.randint(0, len(self.buffer), size=batch_size)\n",
        "\n",
        "        states, actions, next_states, rewards, dones, goals = [], [], [], [], [], []\n",
        "\n",
        "        for i in ind:\n",
        "            s, a, s_, r, d, g = self.buffer[i]\n",
        "            states.append(np.asarray(s))\n",
        "            actions.append(np.asarray(a))\n",
        "            next_states.append(np.asarray(s_))\n",
        "            rewards.append(np.asarray(r))\n",
        "            dones.append(np.asarray(d))\n",
        "            goals.append(np.asarray(g))\n",
        "\n",
        "        return (\n",
        "            torch.FloatTensor(np.array(states)).to(device),\n",
        "            torch.FloatTensor(np.array(actions)).to(device),\n",
        "            torch.FloatTensor(np.array(next_states)).to(device),\n",
        "            torch.FloatTensor(np.array(rewards).reshape(-1, 1)).to(device),\n",
        "            torch.FloatTensor(np.array(dones).reshape(-1, 1)).to(device),\n",
        "            torch.FloatTensor(np.array(goals)).to(device)\n",
        "        )\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)"
      ],
      "metadata": {
        "id": "wTSR0ghzEUsw"
      },
      "execution_count": 197,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Implementation: *DDPG*"
      ],
      "metadata": {
        "id": "HIKXP2zFKN3d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DDPG with HER agent\n",
        "class DDPG_HER:\n",
        "    def __init__(self, state_dim, action_dim, goal_dim, max_action):\n",
        "        # Our input to the actor and critic is state+goal, so we need to account for that in the dimensions\n",
        "        input_dim = state_dim + goal_dim\n",
        "\n",
        "        # The networks\n",
        "        self.actor = Actor(input_dim, action_dim, max_action).to(device)\n",
        "        self.actor_target = copy.deepcopy(self.actor)\n",
        "        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=0.001)\n",
        "\n",
        "        self.critic = Critic(input_dim, action_dim).to(device)\n",
        "        self.critic_target = copy.deepcopy(self.critic)\n",
        "        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=0.001)\n",
        "\n",
        "        self.max_action = max_action # continuous action-values -> this is 1.0 for Fetch\n",
        "        self.replay_buffer = ReplayBuffer()\n",
        "\n",
        "        self.batch_size = 256 # from the fetch paper\n",
        "        self.gamma = 0.98 # arbitrary, I don't see it in the paper\n",
        "        self.tau = 0.95 # from the fetch paper - surprisingly very high\n",
        "\n",
        "    def select_action(self, observation, noise, noise_type=\"Gaussian\"):\n",
        "        \"\"\"Select action based on observation dictionary\"\"\"\n",
        "        # Create input by concatenating state and goal\n",
        "        state = observation[\"observation\"]\n",
        "        goal = observation[\"desired_goal\"]\n",
        "        state_goal = np.concatenate([state, goal])\n",
        "        state_goal = torch.FloatTensor(state_goal.reshape(1, -1)).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            action = self.actor(state_goal).cpu().data.numpy().flatten()\n",
        "\n",
        "        # Add noise for exploration\n",
        "        if noise > 0 and noise_type == \"OU\":\n",
        "          # We check if the subprocess is not already running\n",
        "          if not hasattr(self, 'noise_process'):\n",
        "            self.noise_process = OrnsteinUhlenbeckNoise(\n",
        "                action_dimension=len(action),\n",
        "                mu=0,\n",
        "                theta=0.15, # from the continuous control paper\n",
        "                sigma=0.2   # from the continuous control paper\n",
        "            )\n",
        "            action = action + noise * self.noise_process.sample()\n",
        "\n",
        "        if noise > 0 and noise_type == \"Gaussian\":\n",
        "            action = action + np.random.normal(0, noise, size=action.shape)\n",
        "\n",
        "        return np.clip(action, -self.max_action, self.max_action)\n",
        "\n",
        "    def reset_noise(self):\n",
        "        if hasattr(self, 'noise_process'):\n",
        "            self.noise_process.reset()\n",
        "\n",
        "    def train(self):\n",
        "        if len(self.replay_buffer) < self.batch_size:\n",
        "            return\n",
        "\n",
        "        # Sample from replay buffer\n",
        "        state, action, next_state, reward, done, goal = self.replay_buffer.sample(self.batch_size)\n",
        "\n",
        "        # Compute target Q value\n",
        "        with torch.no_grad():\n",
        "            next_action = self.actor_target(next_state)\n",
        "            target_Q = self.critic_target(next_state, next_action)\n",
        "            target_Q = reward + (1 - done) * self.gamma * target_Q\n",
        "\n",
        "        # Get current Q estimate\n",
        "        current_Q = self.critic(state, action)\n",
        "\n",
        "        # Compute critic loss\n",
        "        critic_loss = F.mse_loss(current_Q, target_Q)\n",
        "\n",
        "        # Optimize the critic\n",
        "        self.critic_optimizer.zero_grad()\n",
        "        critic_loss.backward()\n",
        "        self.critic_optimizer.step()\n",
        "\n",
        "        # Compute actor loss\n",
        "        actor_loss = -self.critic(state, self.actor(state)).mean()\n",
        "\n",
        "        # Optimize the actor\n",
        "        self.actor_optimizer.zero_grad()\n",
        "        actor_loss.backward()\n",
        "        self.actor_optimizer.step()\n",
        "\n",
        "        # Update target networks\n",
        "        for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
        "            target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
        "\n",
        "        for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
        "            target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)"
      ],
      "metadata": {
        "id": "eqZs2BMEEXvl"
      },
      "execution_count": 198,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train Function"
      ],
      "metadata": {
        "id": "zCvFUKdkKTBA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_ddpg_with_her(env, debug = True):\n",
        "    # Set seeds\n",
        "    seed = 5\n",
        "    env.action_space.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    env_name = env.unwrapped.spec.id\n",
        "\n",
        "    # Create agent\n",
        "    obs_dict = env.reset()[0]\n",
        "    state_dim = obs_dict[\"observation\"].shape[0]  # 10\n",
        "    action_dim = env.action_space.shape[0]  # 4\n",
        "    goal_dim = obs_dict[\"desired_goal\"].shape[0]  # 3\n",
        "    max_action = float(env.action_space.high[0])  # 1.0\n",
        "\n",
        "    agent = DDPG_HER(state_dim, action_dim, goal_dim, max_action)\n",
        "\n",
        "    # Training parameters - increased for better learning\n",
        "    n_epochs = 30 # from the fetch paper results we should converge by that time\n",
        "    n_cycles = 50 # from the fetch paper\n",
        "    # In the paper they trained on cpu 19 cores, I don't really understand their calculations so these two are arbitrary\n",
        "    n_episodes = 16\n",
        "    n_timesteps = 100\n",
        "\n",
        "    if debug:\n",
        "      n_epochs = 2\n",
        "      n_cycles = 2\n",
        "      n_episodes = 2\n",
        "      n_timesteps = 2\n",
        "\n",
        "    # For evaluation\n",
        "    success_history = []\n",
        "    # and a typical learning curve\n",
        "    reward_history = []\n",
        "\n",
        "    # Start training\n",
        "    print(\"Starting training...\")\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        successes = 0\n",
        "        epoch_rewards = []  # track rewards for all episodes in this epoch\n",
        "\n",
        "        for cycle in range(n_cycles):\n",
        "            for episode in range(n_episodes):\n",
        "                # Reset environment\n",
        "                obs_dict, _ = env.reset()\n",
        "\n",
        "                # Reset OU exploration noise\n",
        "                agent.reset_noise()\n",
        "\n",
        "                # Initialize tracking\n",
        "                episode_success = False\n",
        "                episode_reward = 0\n",
        "\n",
        "                for t in range(n_timesteps):\n",
        "                    # Select action with noise for exploration\n",
        "                    action = agent.select_action(obs_dict, noise=0.2) # TODO: gradually decay the noise\n",
        "\n",
        "                    # Execute action\n",
        "                    next_obs_dict, reward, terminated, truncated, info = env.step(action)\n",
        "                    done = terminated or truncated\n",
        "\n",
        "                    # Accumulate rewards\n",
        "                    episode_reward += reward\n",
        "\n",
        "                    # Store transition in episode buffer\n",
        "                    agent.replay_buffer.add_episode_step(\n",
        "                        obs_dict, action, next_obs_dict, reward, float(done)\n",
        "                    )\n",
        "\n",
        "                    # Update observations\n",
        "                    obs_dict = next_obs_dict\n",
        "\n",
        "                    # Track success\n",
        "                    if info.get(\"is_success\", 0) > 0:\n",
        "                        episode_success = True\n",
        "\n",
        "                    if done:\n",
        "                        break\n",
        "\n",
        "                # Process episode with HER\n",
        "                agent.replay_buffer.process_episode_with_her(env, k=4) # k=4 means the probability of HER experience replay is 0.8 like in the fetch paper\n",
        "\n",
        "                epoch_rewards.append(episode_reward)\n",
        "\n",
        "                if episode_success:\n",
        "                    successes += 1\n",
        "\n",
        "            # Update policy after each cycle\n",
        "            for _ in range(40):  # Increased updates per cycle\n",
        "                agent.train()\n",
        "\n",
        "        # Calculate success rate and rewards for this epoch\n",
        "        success_rate = successes / (n_cycles * n_episodes)\n",
        "        success_history.append(success_rate)\n",
        "\n",
        "        avg_reward = sum(epoch_rewards) / len(epoch_rewards)\n",
        "        reward_history.append(avg_reward)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{n_epochs}, Success Rate: {success_rate:.2f}\")\n",
        "\n",
        "    # Save trained model\n",
        "    torch.save(agent.actor.state_dict(), f\"{env_name}_actor.pth\")\n",
        "\n",
        "    return agent.actor, success_history, reward_history"
      ],
      "metadata": {
        "id": "hHW4ypdpEdfq"
      },
      "execution_count": 199,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run training with DDPG + HER"
      ],
      "metadata": {
        "id": "1t7rtH_pKmMh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env = create_env()\n",
        "actor, success_history, reward_history = train_ddpg_with_her(env, debug = False)"
      ],
      "metadata": {
        "id": "RFjtGaLkEg3H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1808dbc3-a635-4ba1-b0af-a7d9dd57da6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Observation space: Dict('achieved_goal': Box(-inf, inf, (3,), float64), 'desired_goal': Box(-inf, inf, (3,), float64), 'observation': Box(-inf, inf, (10,), float64))\n",
            "Action space: Box(-1.0, 1.0, (4,), float32)\n",
            "Starting training...\n",
            "Epoch 1/30, Success Rate: 0.00\n",
            "Epoch 2/30, Success Rate: 0.00\n",
            "Epoch 3/30, Success Rate: 0.00\n",
            "Epoch 4/30, Success Rate: 0.03\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load the actor from a file"
      ],
      "metadata": {
        "id": "PZOqo_jnNxcx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_agent(model_path, env):\n",
        "    obs_dict = env.reset()[0]\n",
        "    state_dim = obs_dict[\"observation\"].shape[0]  # 10\n",
        "    action_dim = env.action_space.shape[0]  # 4\n",
        "    goal_dim = obs_dict[\"desired_goal\"].shape[0]  # 3\n",
        "    max_action = float(env.action_space.high[0])  # 1.0\n",
        "\n",
        "    # Create agent with the same architecture\n",
        "    input_dim = state_dim + goal_dim\n",
        "    actor = Actor(input_dim, action_dim, max_action).to(device)\n",
        "\n",
        "    # Load the saved weights\n",
        "    actor.load_state_dict(torch.load(model_path))\n",
        "    actor.eval()  # Set to evaluation mode\n",
        "\n",
        "    return actor"
      ],
      "metadata": {
        "id": "k8VCf1wONwsU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation"
      ],
      "metadata": {
        "id": "O2mIdeJNEkd5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluate Function"
      ],
      "metadata": {
        "id": "IoHNVNtlKt1J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(actor_model, env, n_episodes=10, max_steps=100):\n",
        "    \"\"\"\n",
        "    Evaluates using just an actor model on multiple episodes and creates a single GIF\n",
        "    with episodes playing one after another in sequence.\n",
        "\n",
        "    Args:\n",
        "        actor_model: The trained actor network\n",
        "        env: The environment\n",
        "        n_episodes: Number of episodes to run\n",
        "        max_steps: Maximum steps per episode\n",
        "\n",
        "    Returns:\n",
        "        success_rate: Fraction of successful episodes\n",
        "    \"\"\"\n",
        "    # Storage for sequential frames\n",
        "    all_frames = []\n",
        "    successes = 0\n",
        "    env_name = env.unwrapped.spec.id\n",
        "    device = next(actor_model.parameters()).device  # Get the device of the model\n",
        "\n",
        "    # First get a sample frame to determine dimensions\n",
        "    obs_dict, _ = env.reset()\n",
        "    sample_frame = env.render()\n",
        "    frame_height, frame_width = sample_frame.shape[0], sample_frame.shape[1]\n",
        "\n",
        "    # Run all episodes sequentially\n",
        "    for episode in range(n_episodes):\n",
        "        obs_dict, _ = env.reset()\n",
        "        episode_success = False\n",
        "\n",
        "        # Episode title frame\n",
        "        title_frame = np.zeros((frame_height, frame_width, 3), dtype=np.uint8)\n",
        "        pil_title = Image.fromarray(title_frame)\n",
        "        draw = ImageDraw.Draw(pil_title)\n",
        "        draw.text((frame_width//2 - 80, frame_height//2), f\"Episode {episode+1}\", fill=(255, 255, 255))\n",
        "        all_frames.append(np.array(pil_title))\n",
        "\n",
        "        # 5 blank frames as a pause between episodes\n",
        "        for _ in range(5):\n",
        "            all_frames.append(title_frame.copy())\n",
        "\n",
        "        for t in range(max_steps):\n",
        "            # Render the current state\n",
        "            frame = env.render()\n",
        "\n",
        "            # Add episode number as overlay\n",
        "            pil_frame = Image.fromarray(frame)\n",
        "            draw = ImageDraw.Draw(pil_frame)\n",
        "            draw.text((10, 10), f\"Episode {episode+1}\", fill=(255, 255, 255))\n",
        "            frame = np.array(pil_frame)\n",
        "\n",
        "            # Add frame to full sequence\n",
        "            all_frames.append(frame)\n",
        "\n",
        "            # Select action using actor model directly\n",
        "            state = obs_dict[\"observation\"]\n",
        "            goal = obs_dict[\"desired_goal\"]\n",
        "            state_goal = np.concatenate([state, goal])\n",
        "            state_goal = torch.FloatTensor(state_goal.reshape(1, -1)).to(device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                action = actor_model(state_goal).cpu().data.numpy().flatten()\n",
        "\n",
        "            # Step environment\n",
        "            next_obs_dict, _, terminated, truncated, info = env.step(action)\n",
        "\n",
        "            # Check success\n",
        "            if info.get(\"is_success\", 0) > 0:\n",
        "                episode_success = True\n",
        "\n",
        "            obs_dict = next_obs_dict\n",
        "\n",
        "            if terminated or truncated:\n",
        "                break\n",
        "\n",
        "        # Make sure we have at least one frame from this episode\n",
        "        if len(all_frames) == 0:\n",
        "            all_frames.append(title_frame.copy())\n",
        "\n",
        "        # Get the last frame for this episode\n",
        "        last_frame = all_frames[-1].copy()\n",
        "\n",
        "        # Mark episode result with a summary frame\n",
        "        pil_result = Image.fromarray(last_frame)\n",
        "        draw = ImageDraw.Draw(pil_result)\n",
        "        status = \"SUCCESS\" if episode_success else \"FAILURE\"\n",
        "        color = (0, 255, 0) if episode_success else (255, 0, 0)\n",
        "        # Draw larger text for visibility\n",
        "        draw.text((frame_width//2 - 80, frame_height//2),\n",
        "                  status, fill=color)\n",
        "\n",
        "        result_frame = np.array(pil_result)\n",
        "        all_frames.append(result_frame)\n",
        "\n",
        "        # Add a few still frames at the end of each episode to see the result\n",
        "        for _ in range(15):  # Hold the result for 15 frames\n",
        "            all_frames.append(result_frame.copy())\n",
        "\n",
        "        # Track success\n",
        "        if episode_success:\n",
        "            successes += 1\n",
        "\n",
        "    # Check if all frames have the same shape\n",
        "    shapes = set(frame.shape for frame in all_frames)\n",
        "    if len(shapes) > 1:\n",
        "        print(f\"Warning: Frames have different shapes: {shapes}\")\n",
        "        # Resize all frames to the dimensions of the first frame\n",
        "        target_shape = all_frames[0].shape\n",
        "        for i in range(len(all_frames)):\n",
        "            if all_frames[i].shape != target_shape:\n",
        "                pil_frame = Image.fromarray(all_frames[i])\n",
        "                pil_frame = pil_frame.resize((target_shape[1], target_shape[0]), Image.LANCZOS)\n",
        "                all_frames[i] = np.array(pil_frame)\n",
        "\n",
        "    # Save as GIF\n",
        "    imageio.mimsave(f\"{env_name}_eval.gif\", all_frames, fps=30, loop=0)\n",
        "    print(f\"Saved sequential animation to {env_name}_eval.gif\")\n",
        "\n",
        "    success_rate = successes / n_episodes\n",
        "    print(f\"Evaluation success rate: {success_rate:.2f}\")\n",
        "\n",
        "    return success_rate"
      ],
      "metadata": {
        "id": "cnwxqu6fEpvI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run evaluation and save the gif"
      ],
      "metadata": {
        "id": "QOjq4JNdKzf9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env = create_env()\n",
        "evaluate(actor, env, n_episodes=10)"
      ],
      "metadata": {
        "id": "aXNrLZZXE0Az"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}